# -*- coding: utf-8 -*-
"""Visitor_task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bf28SBCqazGv1GZ9NgSXBC2D1KMGv8JU
"""

# Cell 1: Install dependencies
!pip install insightface opencv-python-headless imutils deep_sort_realtime

!pip install onnxruntime

# Cell 2: Import libraries
import cv2
import insightface
from insightface.app import FaceAnalysis
from deep_sort_realtime.deepsort_tracker import DeepSort
import numpy as np
import uuid

# Cell 3: Initialize face detection, recognition, and DeepSORT tracker
face_app = FaceAnalysis(
    name='buffalo_l',  # Model for RetinaFace + ArcFace
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
)
face_app.prepare(ctx_id=0, det_size=(640, 640))

tracker = DeepSort(max_age=30, n_init=3)

# Cell 4: Helper functions and face database

def draw_face(image, bbox, id_num):
    x1, y1, x2, y2 = [int(i) for i in bbox]
    cv2.rectangle(image, (x1, y1), (x2, y2), (50, 255, 50), 2)
    cv2.putText(image, f'ID: {id_num}', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 255, 50), 2)

face_db = []
def register_or_identify(face_emb):
    # Identify or register new face based on embedding similarity
    for entry in face_db:
        sim = np.dot(face_emb, entry['emb']) / (np.linalg.norm(face_emb) * np.linalg.norm(entry['emb']))
        if sim > 0.6:  # Similarity threshold
            return entry['id']
    # If new face, assign new id
    new_id = str(uuid.uuid4())[:8]
    face_db.append({'id': new_id, 'emb': face_emb})
    return new_id

def video_stream(input_path):
    return cv2.VideoCapture(input_path)

# Cell 5: Main detection, recognition, and tracking loop

from google.colab.patches import cv2_imshow  # Only needed in Colab

video_path = '/content/video_sample1.mp4'  # Replace with RTSP URL for camera, e.g., 'rtsp://username:password@camera_ip'
cap = video_stream(video_path)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    faces = face_app.get(frame)
    bboxes = [f.bbox for f in faces]
    features = [f.embedding for f in faces]

    # DeepSORT expects: [ [ [x1, y1, x2, y2], confidence ], ... ]
    # InsightFace does not provide confidence, so set it manually (e.g., 1.0 for all)
    formatted_bboxes = [ [list(map(float, bbox)), 1.0] for bbox in bboxes ]

    tracks = tracker.update_tracks(formatted_bboxes, frame=frame)   # Now correct input format

    for i, face in enumerate(faces):
        identity_id = register_or_identify(face.embedding)
        draw_face(frame, face.bbox, identity_id)

    cv2_imshow(frame)
    # For demonstration, break after a few frames if desired
    # break  # Uncomment for single frame testing

cap.release()
cv2.destroyAllWindows()

# Cell 5: Main detection, recognition, and tracking loop

from google.colab.patches import cv2_imshow  # Only needed in Colab

video_path = '/content/video_sample1.mp4'  # Replace with RTSP URL for camera, e.g., 'rtsp://username:password@camera_ip'
cap = video_stream(video_path)

frame_count = 0
max_frames = 5   # ðŸ‘ˆ change this to how many frames you want to process

while True:
    ret, frame = cap.read()
    if not ret:
        break

    faces = face_app.get(frame)
    bboxes = [f.bbox for f in faces]
    features = [f.embedding for f in faces]

    # DeepSORT expects: [ [ [x1, y1, x2, y2], confidence ], ... ]
    formatted_bboxes = [ [list(map(float, bbox)), 1.0] for bbox in bboxes ]

    tracks = tracker.update_tracks(formatted_bboxes, frame=frame)

    for i, face in enumerate(faces):
        identity_id = register_or_identify(face.embedding)
        draw_face(frame, face.bbox, identity_id)

    cv2_imshow(frame)

    frame_count += 1
    if frame_count >= max_frames:   # ðŸ‘ˆ stops after set number of frames
        print(f"Stopped after {max_frames} frames.")
        break

cap.release()
cv2.destroyAllWindows()

# @title With DB connection
!pip install insightface opencv-python-headless imutils pymongo deep_sort_realtime

!pip install onnxruntime

import os
import cv2
import datetime
import base64
import numpy as np
import uuid
from pymongo import MongoClient
import insightface
from insightface.app import FaceAnalysis
from deep_sort_realtime.deepsort_tracker import DeepSort

# Initialize face detection and recognition models
face_app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
face_app.prepare(ctx_id=0, det_size=(640, 640))

# Initialize DeepSORT tracker
tracker = DeepSort(max_age=30, n_init=3)

# Use your actual MongoDB Atlas connection string here (replace <12345> with password)
MONGO_CONNECTION_STRING = "mongodb+srv://CV_TASK:12345@cluster-1.1aklqjp.mongodb.net/?retryWrites=true&w=majority&appName=Cluster-1"

client = MongoClient(MONGO_CONNECTION_STRING)
db = client['face_recognition_db']
face_collection = db['faces']
event_collection = db['events_log']

LOG_BASE_DIR = 'logs/entries'
LOG_FILE = 'events.log'

try:
    # This command forces a call to the server and will raise error if connection fails
    server_info = client.server_info()
    print("Successfully connected to MongoDB Atlas.")
    print("Server info:", server_info)
except Exception as e:
    print("Failed to connect to MongoDB Atlas.")
    print("Error:", e)

def encode_image_to_base64(image):
    _, buffer = cv2.imencode('.jpg', image)
    return base64.b64encode(buffer).decode()

def save_face_crop(face_id, image, bbox, event_type):
    date_folder = datetime.datetime.now().strftime('%Y-%m-%d')
    out_dir = os.path.join(LOG_BASE_DIR, date_folder)
    os.makedirs(out_dir, exist_ok=True)
    x1, y1, x2, y2 = map(int, bbox)
    face_crop = image[y1:y2, x1:x2]
    timestamp_str = datetime.datetime.now().strftime('%H%M%S')
    filename = f"{face_id}_{event_type}_{timestamp_str}.jpg"
    full_path = os.path.join(out_dir, filename)
    cv2.imwrite(full_path, face_crop)
    return full_path, face_crop

def log_event(event_type, face_id, image_path=None):
    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()
    log_line = f"{timestamp}, {event_type}, {face_id}, {image_path if image_path else ''}"
    with open(LOG_FILE, 'a') as f:
        f.write(log_line + "\n")
    event_collection.insert_one({"timestamp": timestamp, "event_type": event_type, "face_id": face_id, "image_path": image_path or ''})

def register_face_in_db(face_id, embedding, bbox, face_crop):
    timestamp = datetime.datetime.now(datetime.timezone.utc)
    face_doc = {
        "face_id": face_id,
        "embedding": embedding.tolist(),
        "bbox": list(map(int, bbox)),
        "timestamp": timestamp,
        "image_base64": encode_image_to_base64(face_crop),
    }
    face_collection.insert_one(face_doc)

face_db = []

def register_or_identify(face_emb, bbox=None, image=None, event_type='entry'):
    for entry in face_db:
        sim = np.dot(face_emb, entry['emb']) / (np.linalg.norm(face_emb) * np.linalg.norm(entry['emb']))
        if sim > 0.6:
            return entry['id']
    new_id = str(uuid.uuid4())[:8]
    face_db.append({'id': new_id, 'emb': face_emb})
    if bbox is not None and image is not None:
        img_path, face_crop = save_face_crop(new_id, image, bbox, event_type)
        register_face_in_db(new_id, face_emb, bbox, face_crop)
        log_event(event_type, new_id, img_path)
    return new_id

def draw_face(image, bbox, id_num):
    x1, y1, x2, y2 = [int(i) for i in bbox]
    cv2.rectangle(image, (x1, y1), (x2, y2), (50, 255, 50), 2)
    cv2.putText(image, f'ID: {id_num}', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 255, 50), 2)

def draw_count(image, count):
    text = f"Unique People Present: {count}"
    cv2.putText(image, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 200, 200), 2)

from google.colab.patches import cv2_imshow

def main(video_path, max_frames=None):
    cap = cv2.VideoCapture(video_path)
    active_face_ids = set()
    frame_count = 0
    processed_frames = []
    all_unique_ids = set()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        if max_frames is not None and frame_count > max_frames:
            print(f"Reached max frames: {max_frames}. Stopping.")
            break

        faces = face_app.get(frame)
        bboxes = [list(map(float, f.bbox)) for f in faces]
        formatted_bboxes = [[bbox, 1.0] for bbox in bboxes]

        tracks = tracker.update_tracks(formatted_bboxes, frame=frame)
        current_face_ids = set()

        for face in faces:
            identity_id = register_or_identify(face.embedding, bbox=face.bbox, image=frame, event_type='entry')
            current_face_ids.add(identity_id)
            all_unique_ids.add(identity_id)
            draw_face(frame, face.bbox, identity_id)

        exited_ids = active_face_ids - current_face_ids
        for ex_id in exited_ids:
            log_event('exit', ex_id)

        active_face_ids = current_face_ids

        draw_count(frame, len(current_face_ids))  # Overlay count text
        processed_frames.append(frame.copy())

        cv2_imshow(frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("Stopped by user.")
            break

    cap.release()
    cv2.destroyAllWindows()

    output_file = "/content/processed_output.mp4"
    save_processed_video(processed_frames, output_file)

    print("Total unique visitors detected:", len(all_unique_ids))
    return output_file

import cv2
from IPython.display import HTML
from base64 import b64encode

def save_processed_video(frames, output_path, fps=20):
    height, width, _ = frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID'
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    for frame in frames:
        out.write(frame)
    out.release()

def display_video(path):
    mp4 = open(path,'rb').read()
    data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
    return HTML(f"""
    <video width=600 controls>
        <source src="{data_url}" type="video/mp4">
    </video>
    """)

