{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Bow2uBXUtbuL",
        "outputId": "0e6bcf0d-a4de-4597-b12b-38b9c7521890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.203-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting insightface\n",
            "  Downloading insightface-0.7.3.tar.gz (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.12/dist-packages (0.5.4)\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting deep_sort_realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting onnx (from insightface)\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from insightface) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from insightface) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from insightface) (0.25.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (from insightface) (1.13)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from insightface) (3.0.12)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (from insightface) (2.0.8)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.12/dist-packages (from insightface) (3.16.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations->insightface) (2.11.9)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations->insightface) (0.0.24)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations->insightface) (4.0.11)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations->insightface) (6.5.3)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->insightface) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx->insightface) (0.5.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable->insightface) (0.2.13)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->insightface) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->insightface) (2025.9.9)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->insightface) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->insightface) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->insightface) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->insightface) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.203-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.15.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: insightface\n",
            "  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for insightface: filename=insightface-0.7.3-cp312-cp312-linux_x86_64.whl size=1070140 sha256=59bf785c6e1edfad1ea09244448a507dd5859cc5ab1d91d25ebba1cc30cd464c\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/3c/e2/6d4815e8a8b33a2006554d65ce0d1f973e768f4c7a222fa675\n",
            "Successfully built insightface\n",
            "Installing collected packages: dnspython, pymongo, onnx, deep_sort_realtime, ultralytics-thop, insightface, ultralytics\n",
            "Successfully installed deep_sort_realtime-1.3.2 dnspython-2.8.0 insightface-0.7.3 onnx-1.19.0 pymongo-4.15.1 ultralytics-8.3.203 ultralytics-thop-2.0.17\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch torchvision torchaudio\n",
        "!pip install -U ultralytics insightface opencv-python-headless imutils pymongo deep_sort_realtime\n",
        "!pip install onnxruntime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import datetime\n",
        "import base64\n",
        "import numpy as np\n",
        "import uuid\n",
        "from pymongo import MongoClient\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import insightface\n",
        "from insightface.app import FaceAnalysis\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "\n",
        "# Initialize YOLOv8 model\n",
        "yolo_model = YOLO(\"yolov8n.pt\")\n",
        "if torch.cuda.is_available():\n",
        "    yolo_model.to('cuda')  # Move model to GPU if CUDA is available\n",
        "\n",
        "# Initialize InsightFace with GPU support\n",
        "face_app = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "face_app.prepare(ctx_id=0)  # ctx_id=0 sets GPU for InsightFace\n",
        "\n",
        "# Initialize DeepSORT tracker\n",
        "tracker = DeepSort(max_age=30, n_init=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LRDjxZwCtiIe",
        "outputId": "e1d670ab-ae90-4629-a05e-25b9d72fb021"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 62.6MB/s 0.1s\n",
            "download_path: /root/.insightface/models/buffalo_l\n",
            "Downloading /root/.insightface/models/buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 281857/281857 [00:04<00:00, 64700.98KB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:123: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Your MongoDB Atlas connection string (replace password accordingly)\n",
        "MONGO_CONNECTION_STRING = \"mongodb+srv://CV_TASK:12345@cluster-1.1aklqjp.mongodb.net/?retryWrites=true&w=majority&appName=Cluster-1\"\n",
        "\n",
        "client = MongoClient(MONGO_CONNECTION_STRING)\n",
        "db = client['face_recognition_db']\n",
        "face_collection = db['faces']\n",
        "event_collection = db['events_log']\n",
        "\n",
        "LOG_BASE_DIR = '/content/drive/MyDrive/Visitor_Task/logs/entries'\n",
        "LOG_FILE = '/content/drive/MyDrive/Visitor_Task/events.log'\n"
      ],
      "metadata": {
        "id": "UzaEXNY-tiLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_image_to_base64(image):\n",
        "    _, buffer = cv2.imencode('.jpg', image)\n",
        "    return base64.b64encode(buffer).decode()\n",
        "\n",
        "def save_face_crop(face_id, image, bbox, event_type):\n",
        "    date_folder = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "    out_dir = os.path.join(LOG_BASE_DIR, date_folder)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    x1, y1, x2, y2 = map(int, bbox)\n",
        "    face_crop = image[y1:y2, x1:x2]\n",
        "    timestamp_str = datetime.datetime.now().strftime('%H%M%S')\n",
        "    filename = f\"{face_id}_{event_type}_{timestamp_str}.jpg\"\n",
        "    full_path = os.path.join(out_dir, filename)\n",
        "    cv2.imwrite(full_path, face_crop)\n",
        "    return full_path, face_crop\n",
        "\n",
        "def log_event(event_type, face_id, image_path=None):\n",
        "    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
        "    log_line = f\"{timestamp}, {event_type}, {face_id}, {image_path if image_path else ''}\"\n",
        "    with open(LOG_FILE, 'a') as f:\n",
        "        f.write(log_line + \"\\n\")\n",
        "    event_collection.insert_one({\"timestamp\": timestamp, \"event_type\": event_type, \"face_id\": face_id, \"image_path\": image_path or ''})\n",
        "\n",
        "def register_face_in_db(face_id, embedding, bbox, face_crop):\n",
        "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
        "    face_doc = {\n",
        "        \"face_id\": face_id,\n",
        "        \"embedding\": embedding.tolist(),\n",
        "        \"bbox\": list(map(int, bbox)),\n",
        "        \"timestamp\": timestamp,\n",
        "        \"image_base64\": encode_image_to_base64(face_crop),\n",
        "    }\n",
        "    face_collection.insert_one(face_doc)\n"
      ],
      "metadata": {
        "id": "vW9FbFsUtiOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_db = []\n",
        "\n",
        "def register_or_identify(face_emb, bbox=None, image=None, event_type='entry'):\n",
        "    for entry in face_db:\n",
        "        sim = np.dot(face_emb, entry['emb']) / (np.linalg.norm(face_emb) * np.linalg.norm(entry['emb']))\n",
        "        if sim > 0.6:\n",
        "            return entry['id']\n",
        "    new_id = str(uuid.uuid4())[:8]\n",
        "    face_db.append({'id': new_id, 'emb': face_emb})\n",
        "    if bbox is not None and image is not None:\n",
        "        img_path, face_crop = save_face_crop(new_id, image, bbox, event_type)\n",
        "        register_face_in_db(new_id, face_emb, bbox, face_crop)\n",
        "        log_event(event_type, new_id, img_path)\n",
        "    return new_id\n",
        "\n",
        "def draw_face(image, bbox, id_num):\n",
        "    x1, y1, x2, y2 = [int(i) for i in bbox]\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (50, 255, 50), 2)\n",
        "    cv2.putText(image, f'ID: {id_num}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 255, 50), 2)\n",
        "\n",
        "def draw_count(image, count):\n",
        "    text = f\"Unique People Present: {count}\"\n",
        "    cv2.putText(image, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 200, 200), 2)\n"
      ],
      "metadata": {
        "id": "tPm7svd4t0sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    active_face_ids = set()\n",
        "    all_unique_ids = set()\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    output_file = \"/content/drive/MyDrive/Visitor_Task/Output/processed_output.mp4\"\n",
        "    out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    detection_interval = 3\n",
        "    input_width, input_height = 640, 360\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_count += 1\n",
        "\n",
        "        if frame_count % detection_interval == 0:\n",
        "            small_frame = cv2.resize(frame, (input_width, input_height))\n",
        "            results = yolo_model(small_frame)\n",
        "            result = results[0]\n",
        "\n",
        "            faces_bboxes = []\n",
        "            for bbox_tensor, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):\n",
        "                x1, y1, x2, y2 = bbox_tensor.cpu().numpy()\n",
        "                x1 = int(x1 * frame.shape[1] / input_width)\n",
        "                x2 = int(x2 * frame.shape[1] / input_width)\n",
        "                y1 = int(y1 * frame.shape[0] / input_height)\n",
        "                y2 = int(y2 * frame.shape[0] / input_height)\n",
        "                conf = conf.cpu().item()\n",
        "                cls = int(cls.cpu().item())\n",
        "                if cls == 0 and conf > 0.5:\n",
        "                    faces_bboxes.append([x1, y1, x2, y2])\n",
        "\n",
        "            current_face_ids = set()\n",
        "            for bbox in faces_bboxes:\n",
        "                x1, y1, x2, y2 = map(int, bbox)\n",
        "                face_crop = frame[y1:y2, x1:x2]\n",
        "                insightface_results = face_app.get(face_crop)\n",
        "                if insightface_results:\n",
        "                    face_embedding = insightface_results[0].embedding\n",
        "                    identity_id = register_or_identify(face_embedding, bbox=bbox, image=frame, event_type='entry')\n",
        "                    current_face_ids.add(identity_id)\n",
        "                    all_unique_ids.add(identity_id)\n",
        "                    draw_face(frame, bbox, identity_id)\n",
        "\n",
        "            exited_ids = active_face_ids - current_face_ids\n",
        "            for ex_id in exited_ids:\n",
        "                log_event('exit', ex_id)\n",
        "\n",
        "            active_face_ids = current_face_ids\n",
        "        else:\n",
        "            # Could update tracker or just skip face update during skipped frames\n",
        "            pass\n",
        "\n",
        "        draw_count(frame, len(active_face_ids))\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(\"Total unique visitors detected in video:\", len(all_unique_ids))\n",
        "    return output_file\n"
      ],
      "metadata": {
        "id": "da3f71-vt0vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/drive/MyDrive/Visitor_Task/Input/video_sample1.mp4\"\n",
        "output_path = main(video_path)\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def display_video(path):\n",
        "    mp4 = open(path, 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(f\"\"\"\n",
        "    <video width=600 controls>\n",
        "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\")\n",
        "\n",
        "display_video(output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTQKheEiEQ2Y",
        "outputId": "7d4f4c40-55c8-4410-9301-e5f66cc632f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 24 persons, 1 handbag, 50.6ms\n",
            "Speed: 2.3ms preprocess, 50.6ms inference, 53.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 4 handbags, 11.6ms\n",
            "Speed: 1.6ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 2 handbags, 18.2ms\n",
            "Speed: 2.8ms preprocess, 18.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 3 handbags, 10.8ms\n",
            "Speed: 1.7ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 3 handbags, 13.9ms\n",
            "Speed: 2.1ms preprocess, 13.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 14.0ms\n",
            "Speed: 1.6ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 2 handbags, 13.6ms\n",
            "Speed: 2.6ms preprocess, 13.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 4 handbags, 8.5ms\n",
            "Speed: 1.6ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 3 handbags, 13.2ms\n",
            "Speed: 2.2ms preprocess, 13.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 2 handbags, 10.9ms\n",
            "Speed: 1.6ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 backpack, 2 handbags, 14.4ms\n",
            "Speed: 2.3ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 24.7ms\n",
            "Speed: 4.7ms preprocess, 24.7ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 backpack, 15.8ms\n",
            "Speed: 1.7ms preprocess, 15.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 2 handbags, 9.1ms\n",
            "Speed: 1.8ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 14.7ms\n",
            "Speed: 1.6ms preprocess, 14.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 17.3ms\n",
            "Speed: 3.6ms preprocess, 17.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 14.1ms\n",
            "Speed: 2.0ms preprocess, 14.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 10.8ms\n",
            "Speed: 2.2ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 backpack, 13.3ms\n",
            "Speed: 2.5ms preprocess, 13.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 backpack, 9.2ms\n",
            "Speed: 1.6ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 backpack, 23.7ms\n",
            "Speed: 3.8ms preprocess, 23.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 backpack, 8.6ms\n",
            "Speed: 2.6ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 27.6ms\n",
            "Speed: 3.7ms preprocess, 27.6ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 backpack, 9.0ms\n",
            "Speed: 2.1ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 1 backpack, 13.2ms\n",
            "Speed: 1.7ms preprocess, 13.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 10.3ms\n",
            "Speed: 1.7ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 14.5ms\n",
            "Speed: 1.7ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 10.1ms\n",
            "Speed: 2.7ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 backpack, 22.5ms\n",
            "Speed: 5.7ms preprocess, 22.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 handbag, 10.0ms\n",
            "Speed: 1.6ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 9.6ms\n",
            "Speed: 2.8ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 handbag, 12.6ms\n",
            "Speed: 1.9ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 1 potted plant, 16.3ms\n",
            "Speed: 2.1ms preprocess, 16.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 9.2ms\n",
            "Speed: 1.6ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 14.2ms\n",
            "Speed: 1.7ms preprocess, 14.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 2 handbags, 10.0ms\n",
            "Speed: 2.9ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 22.6ms\n",
            "Speed: 5.2ms preprocess, 22.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 10.2ms\n",
            "Speed: 1.6ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 18.3ms\n",
            "Speed: 4.9ms preprocess, 18.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 8.0ms\n",
            "Speed: 2.4ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 13.8ms\n",
            "Speed: 2.1ms preprocess, 13.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 21.7ms\n",
            "Speed: 1.6ms preprocess, 21.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 1 handbag, 13.3ms\n",
            "Speed: 2.1ms preprocess, 13.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 handbag, 9.6ms\n",
            "Speed: 1.6ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 handbag, 9.1ms\n",
            "Speed: 4.6ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 3 handbags, 15.7ms\n",
            "Speed: 3.6ms preprocess, 15.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 backpack, 2 handbags, 13.2ms\n",
            "Speed: 1.7ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 backpack, 3 handbags, 9.8ms\n",
            "Speed: 1.6ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 3 handbags, 13.5ms\n",
            "Speed: 2.8ms preprocess, 13.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 handbag, 8.1ms\n",
            "Speed: 2.0ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 handbag, 16.8ms\n",
            "Speed: 1.7ms preprocess, 16.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 16.1ms\n",
            "Speed: 2.0ms preprocess, 16.1ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 13.6ms\n",
            "Speed: 2.0ms preprocess, 13.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 9.6ms\n",
            "Speed: 1.7ms preprocess, 9.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 34 persons, 1 handbag, 14.0ms\n",
            "Speed: 2.7ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 32 persons, 1 handbag, 9.0ms\n",
            "Speed: 2.2ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 33 persons, 1 handbag, 18.6ms\n",
            "Speed: 3.4ms preprocess, 18.6ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 31 persons, 1 handbag, 11.5ms\n",
            "Speed: 1.7ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 30 persons, 1 handbag, 1 sports ball, 10.4ms\n",
            "Speed: 2.8ms preprocess, 10.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 26 persons, 2 handbags, 9.4ms\n",
            "Speed: 1.9ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 1 skateboard, 12.5ms\n",
            "Speed: 1.6ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 1 handbag, 10.0ms\n",
            "Speed: 1.6ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 25 persons, 1 handbag, 31.3ms\n",
            "Speed: 3.0ms preprocess, 31.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 28 persons, 15.6ms\n",
            "Speed: 1.6ms preprocess, 15.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 1 backpack, 10.0ms\n",
            "Speed: 3.7ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 27 persons, 2 backpacks, 9.9ms\n",
            "Speed: 2.6ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 29 persons, 14.3ms\n",
            "Speed: 2.3ms preprocess, 14.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_rtsp_stream(rtsp_url):\n",
        "    cap = cv2.VideoCapture(rtsp_url)\n",
        "    active_face_ids = set()\n",
        "    all_unique_ids = set()\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if fps == 0:\n",
        "        fps = 20  # Default FPS if stream does not provide it\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    output_file = \"/content/processed_output_live.mp4\"\n",
        "    out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    detection_interval = 3\n",
        "    input_width, input_height = 640, 360\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(\"Stream ended or no frame received.\")\n",
        "            break\n",
        "        frame_count += 1\n",
        "\n",
        "        if frame_count % detection_interval == 0:\n",
        "            small_frame = cv2.resize(frame, (input_width, input_height))\n",
        "            results = yolo_model(small_frame)\n",
        "            result = results[0]\n",
        "\n",
        "            faces_bboxes = []\n",
        "            for bbox_tensor, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):\n",
        "                x1, y1, x2, y2 = bbox_tensor.cpu().numpy()\n",
        "                x1 = int(x1 * frame.shape[1] / input_width)\n",
        "                x2 = int(x2 * frame.shape[1] / input_width)\n",
        "                y1 = int(y1 * frame.shape[0] / input_height)\n",
        "                y2 = int(y2 * frame.shape[0] / input_height)\n",
        "                conf = conf.cpu().item()\n",
        "                cls = int(cls.cpu().item())\n",
        "                if cls == 0 and conf > 0.5:\n",
        "                    faces_bboxes.append([x1, y1, x2, y2])\n",
        "\n",
        "            current_face_ids = set()\n",
        "            for bbox in faces_bboxes:\n",
        "                x1, y1, x2, y2 = map(int, bbox)\n",
        "                face_crop = frame[y1:y2, x1:x2]\n",
        "                insightface_results = face_app.get(face_crop)\n",
        "                if insightface_results:\n",
        "                    face_embedding = insightface_results[0].embedding\n",
        "                    identity_id = register_or_identify(face_embedding, bbox=bbox, image=frame, event_type='entry')\n",
        "                    current_face_ids.add(identity_id)\n",
        "                    all_unique_ids.add(identity_id)\n",
        "                    draw_face(frame, bbox, identity_id)\n",
        "\n",
        "            exited_ids = active_face_ids - current_face_ids\n",
        "            for ex_id in exited_ids:\n",
        "                log_event('exit', ex_id)\n",
        "\n",
        "            active_face_ids = current_face_ids\n",
        "        else:\n",
        "            # Optionally update tracker without detection here if applicable\n",
        "            pass\n",
        "\n",
        "        draw_count(frame, len(active_face_ids))\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(\"Total unique visitors detected in live stream:\", len(all_unique_ids))\n",
        "    return output_file\n"
      ],
      "metadata": {
        "id": "h8ECrqR4SrOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rtsp_url = \"rtsp://username:password@ipaddress:port/stream\"\n",
        "output_path = process_rtsp_stream(rtsp_url)\n"
      ],
      "metadata": {
        "id": "YcLUdda2Srp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gU2s-rD9Sr3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}